{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import nltk\n",
    "import gensim\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_processing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/corona_6_9_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./data/corona_6_12_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#중복 댓글 제거\n",
    "df.drop_duplicates(['contents'],keep='first',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(505743, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>...1</th>\n",
       "      <th>contents</th>\n",
       "      <th>modTime</th>\n",
       "      <th>sympathyCount</th>\n",
       "      <th>antipathyCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>엄지척 2개를 드립니다.</td>\n",
       "      <td>2020-03-02 03:33:38</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>대기업이 대기업답게 국가적 위기에 통큰 행보 감사합니다.</td>\n",
       "      <td>2020-03-02 03:34:36</td>\n",
       "      <td>253</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14</td>\n",
       "      <td>문재인씨, 뭐 느낀것 없냐? 마스크 하나 제대로 못하는 주제에....삶은 소대가리라...</td>\n",
       "      <td>2020-03-02 04:00:17</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18</td>\n",
       "      <td>정부보다 낫네요! 감사합니다.</td>\n",
       "      <td>2020-03-02 04:02:34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24</td>\n",
       "      <td>다른 대기업들도 동참 해주세요~ 대구 경북 국민들 구해주세요~</td>\n",
       "      <td>2020-03-02 04:07:10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264684</th>\n",
       "      <td>264685</td>\n",
       "      <td>355893.0</td>\n",
       "      <td>41</td>\n",
       "      <td>지나가는 개가 웃겠다 20대 국회에 출석율 꼴지인 당신이 할말은 아니지 그렇게 오만...</td>\n",
       "      <td>2020-05-29 02:24:56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264685</th>\n",
       "      <td>264686</td>\n",
       "      <td>355894.0</td>\n",
       "      <td>42</td>\n",
       "      <td>니가 왜 5 18  유공자냐</td>\n",
       "      <td>2020-05-29 02:26:04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264686</th>\n",
       "      <td>264687</td>\n",
       "      <td>355895.0</td>\n",
       "      <td>43</td>\n",
       "      <td>운동권 출신의 거지근성이 대한민국을 말아먹는구나</td>\n",
       "      <td>2020-05-29 02:26:28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264687</th>\n",
       "      <td>264688</td>\n",
       "      <td>355896.0</td>\n",
       "      <td>44</td>\n",
       "      <td>당신은  516때 뭘 하셧는가요? 유공자? 무공자? 진실하지 못한자가 뭘 남탓하는지...</td>\n",
       "      <td>2020-05-29 02:47:55</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264688</th>\n",
       "      <td>264689</td>\n",
       "      <td>355897.0</td>\n",
       "      <td>45</td>\n",
       "      <td>20대 국회 아닐지도요\\r\\r\\n미통닭은 아직도 전두환 박정희시대에 살고 있는 사람들이죠</td>\n",
       "      <td>2020-05-29 03:01:03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505743 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0        X1  ...1  \\\n",
       "0                1       1.0     2   \n",
       "1                2       3.0     4   \n",
       "2                3       8.0    14   \n",
       "3                4      12.0    18   \n",
       "4                5      18.0    24   \n",
       "...            ...       ...   ...   \n",
       "264684      264685  355893.0    41   \n",
       "264685      264686  355894.0    42   \n",
       "264686      264687  355895.0    43   \n",
       "264687      264688  355896.0    44   \n",
       "264688      264689  355897.0    45   \n",
       "\n",
       "                                                 contents  \\\n",
       "0                                           엄지척 2개를 드립니다.   \n",
       "1                         대기업이 대기업답게 국가적 위기에 통큰 행보 감사합니다.   \n",
       "2       문재인씨, 뭐 느낀것 없냐? 마스크 하나 제대로 못하는 주제에....삶은 소대가리라...   \n",
       "3                                        정부보다 낫네요! 감사합니다.   \n",
       "4                      다른 대기업들도 동참 해주세요~ 대구 경북 국민들 구해주세요~   \n",
       "...                                                   ...   \n",
       "264684  지나가는 개가 웃겠다 20대 국회에 출석율 꼴지인 당신이 할말은 아니지 그렇게 오만...   \n",
       "264685                                    니가 왜 5 18  유공자냐   \n",
       "264686                         운동권 출신의 거지근성이 대한민국을 말아먹는구나   \n",
       "264687  당신은  516때 뭘 하셧는가요? 유공자? 무공자? 진실하지 못한자가 뭘 남탓하는지...   \n",
       "264688  20대 국회 아닐지도요\\r\\r\\n미통닭은 아직도 전두환 박정희시대에 살고 있는 사람들이죠   \n",
       "\n",
       "                    modTime  sympathyCount  antipathyCount  \n",
       "0       2020-03-02 03:33:38            640               1  \n",
       "1       2020-03-02 03:34:36            253               0  \n",
       "2       2020-03-02 04:00:17             17               6  \n",
       "3       2020-03-02 04:02:34              1               0  \n",
       "4       2020-03-02 04:07:10              1               0  \n",
       "...                     ...            ...             ...  \n",
       "264684  2020-05-29 02:24:56              1               1  \n",
       "264685  2020-05-29 02:26:04              1               0  \n",
       "264686  2020-05-29 02:26:28              2               1  \n",
       "264687  2020-05-29 02:47:55              0               1  \n",
       "264688  2020-05-29 03:01:03              0               0  \n",
       "\n",
       "[505743 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사 추출기(Komoran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words 지정\n",
    "c_stopwords = pd.read_excel('./stopwords/stopwords.xlsx')\n",
    "my_stopwords = c_stopwords['stopwords'].tolist()\n",
    "stopwords = my_stopwords + ['괜히','또또','사방','려면','다해','왜또','부터','지금','가가','가가호호','힌다','지라','그냥','이나','면서','진짜','정말','이제','때문','신들','안중','통령']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_path = './stopwords/user_dict.txt'\n",
    "komoran = Komoran(userdic = non_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['고개', '겸손', '대통령님', '응원']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komoran.nouns('매번 고개숙이고 겸손한 대통령님..항상 응원합니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출 함수\n",
    "def get_nouns(corpus,stopwords=stopwords):\n",
    "    nouns_list = list()\n",
    "    for text in corpus:\n",
    "        try : \n",
    "            text = str(text)\n",
    "            text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', text)\n",
    "            text = re.sub(\"[^가-힣A-Za-z]\",\" \", text)\n",
    "            text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "            nouns = komoran.nouns(text)\n",
    "            nouns = [word for word in nouns if len(word) > 1 and word not in stopwords]\n",
    "            nouns_list.append(nouns)\n",
    "        except : \n",
    "            nouns_list.append('error')\n",
    "    return nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_corpus_v2 = get_nouns(df['contents'],stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "total_df['Article_token_v2'] = news_corpus_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(total_df['Article_token_v2']  , min_count=3, window=5, iter=100, size=100, workers=4, sg=1)\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"./Word2vec/word2vec_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load('./Word2vec/word2vec_final.model')\n",
    "word_model = model.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_keyword = ['토착왜구당','황고환','황교활','일베','쪽빠리','네일베', '미통닭', '틀딱', '박그네','미똥당','일베충','친일','토왜당','친일파','수꼴','보수꼴통','핑크당']\n",
    "bad_keyword = ['공산당','빨갱이','달창','문재앙','문천지','대깨문','좌파','지령', '북괴','짱깨','조선족','문가', '문씨', '문쩝쩝',  '빨갱이정권', '공산당정권', '이낙엽', '문페렴', '문폐렴','문빨갱이','문죄앙','문어벙','어벙이', '간경화', '페미정부', '구라미터', '조작미터',  '박원숭이', '반일선동', '문좌인', '문족', \n",
    "'개재앙', '문통이', '문재앙폐렴', '민좆당', '문여적']\n",
    "total_keyword = good_keyword+bad_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어간 거리 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_distance(model,lists):\n",
    "    #extract indices\n",
    "    index_lists = []\n",
    "    for i in lists:\n",
    "        inf = model.wv.index2word.index(i) \n",
    "        index_lists.append(inf)\n",
    "    # cal word2vec matrix using kernel\n",
    "    word2vec_mat = model.wv.syn0\n",
    "    distance = pairwise_distances(word2vec_mat , metric='euclidean')\n",
    "    dis_to_ker = rbf_kernel(distance,gamma =0.01)\n",
    "    kernal_matrix = dis_to_ker[:,index_lists]\n",
    "    return pd.DataFrame(kernal_matrix,columns = lists,index = model.wv.index2word).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_matrix = matrix_distance(model,total_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Word2vec/word2vec_matrix.pickle','wb') as f:\n",
    "    pickle.dump(word2vec_matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22781, 53)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec, tdm 내적 시키기 위해 데이터들을 5만개씩 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tdm1 = df.iloc[:50000,:]\n",
    "df_tdm2 = df.iloc[50000:100000,:]\n",
    "df_tdm3 = df.iloc[100000:150000,:]\n",
    "df_tdm4 = df.iloc[150000:200000,:]\n",
    "df_tdm5 = df.iloc[200000:250000,:]\n",
    "df_tdm6 = df.iloc[250000:300000,:]\n",
    "df_tdm7 = df.iloc[300000:350000,:]\n",
    "df_tdm8 = df.iloc[350000:400000,:]\n",
    "df_tdm9 = df.iloc[400000:450000,:]\n",
    "df_tdm10 = df.iloc[450000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, pos=[\"NNP\",\"NNG\",\"NNB\",\"NP\",\"NR\"], stopword=stopwords):\n",
    "    return [\n",
    "        word for word, tag in komoran.pos(\n",
    "            text    \n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopwords\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  word2vec,tdm 내적시켜 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tdm(df,word2vec_matrix,good_keyword,bad_keyword,stopwords):\n",
    "  #관심 키워드 설정\n",
    "    total_keyword = good_keyword+bad_keyword\n",
    "  # 명사추출 vectorizer\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenizer,min_df=3)\n",
    "  # 인터넷 명사 사전 load\n",
    "    non_path = '/gdrive/My Drive/Word2vec/stopwords/user_dict.txt'\n",
    "    komoran = Komoran(userdic = non_path)\n",
    "  #명사추출\n",
    "    inf_nouns = get_nouns(df['contents'],stopwords=stopwords)\n",
    "  # tdm 추출\n",
    "    inf_tdm = pd.DataFrame()\n",
    "    inf_tdm['nouns'] = inf_nouns\n",
    "    inf_tdm['nouns_to_str']=inf_tdm['nouns'].str.join(' ')\n",
    "    tdm = vectorizer.fit_transform(inf_tdm['nouns_to_str'])\n",
    "    tdm_features = vectorizer.get_feature_names()\n",
    "    print(type(tdm))\n",
    "    tdm_df = pd.DataFrame(tdm.toarray(),columns = tdm_features)\n",
    "      # word2vec matrix와 tdm 교집합 명사 \n",
    "    word2vec_matrix = word2vec_matrix.reindex(tdm_features)\n",
    "    word2vec_matrix.dropna(inplace=True)\n",
    "    tdm_df = tdm_df.reindex(list(word2vec_matrix.index),axis=1)\n",
    "    tdm_df.dropna(inplace=True)\n",
    "      # word2vec tdm 내적\n",
    "    score_df = pd.DataFrame(np.dot(tdm_df,word2vec_matrix),columns=total_keyword)\n",
    "      # 내적된 matrix good/bad로 나눔\n",
    "    inf_good = score_df[good_keyword]\n",
    "    inf_bad = score_df[bad_keyword]\n",
    "      #상위 5개씩 추출\n",
    "    inf_good_sum = []\n",
    "    for i in np.sort(inf_good):\n",
    "        max_5 = (i[-5:].sum())/5\n",
    "        inf_good_sum.append(max_5)\n",
    "\n",
    "    inf_bad_sum = []\n",
    "    for i in np.sort(inf_bad):\n",
    "        max_5 = (i[-5:].sum())/5\n",
    "        inf_bad_sum.append(max_5)\n",
    "    score_df['good_score'] = inf_good_sum\n",
    "    score_df['bad_score'] = inf_bad_sum\n",
    "      # 계산된 score 바탕으로 good/bad 분류\n",
    "    df['good_score'] = score_df['good_score'].values\n",
    "    df['bad_score'] = score_df['bad_score'].values\n",
    "    df['Judgement'] = np.where((df['bad_score']) > (df['good_score']),'bad','good')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_tdm1,df_tdm2,df_tdm3,df_tdm4,df_tdm5,df_tdm6,df_tdm7,df_tdm8,df_tdm9,df_tdm10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "while i <11:\n",
    "    print(i)\n",
    "    for df in df_list:\n",
    "        tdm = make_tdm(df,word2vec_matrix,good_keyword=good_keyword,bad_keyword=bad_keyword,stopwords=stopwords)\n",
    "        tdm.to_excel('/gdrive/My Drive/Word2vec/result/tdm{}.xlsx'.format(i))\n",
    "        i+=1\n",
    "        print('save excel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tdm data merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.listdir(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_df = pd.DataFrame()\n",
    "for name in filename:\n",
    "    full_name = os.path.join(filepath,name)\n",
    "    print(full_name)\n",
    "    df = pd.read_excel(full_name)\n",
    "    inf_df = pd.concat([df,inf_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./result'):\n",
    "    os.makedirs('./result')\n",
    "inf_df.to_csv('./result/total_tdm.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./result/total_tdm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Judgement'] = np.where(df['Judgement'] == 'bad',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = df['contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['Judgement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x,ts_x,tr_y,ts_y = train_test_split(analysis_data,target, test_size=0.1, stratify=target,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([tr_x,tr_y],axis=1)\n",
    "\n",
    "test = pd.concat([ts_x,ts_y],axis=1)\n",
    "\n",
    "test.to_csv('./result/test.csv',encoding='utf-8-sig')\n",
    "\n",
    "train.to_csv('./result/train.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
